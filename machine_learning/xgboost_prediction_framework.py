# 1. ç¯å¢ƒå‡†å¤‡ä¸æ•°æ®è·å–

import yfinance as yf
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# æŠ€æœ¯æŒ‡æ ‡è®¡ç®—åº“
import ta

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb

# å¯è§†åŒ–
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def fetch_stock_data(ticker, start_date, end_date):
    """
    è·å–è‚¡ç¥¨å†å²æ•°æ®
    """
    print(f"æ­£åœ¨ä¸‹è½½ {ticker} æ•°æ® ({start_date} åˆ° {end_date})...")
    stock = yf.download(ticker, start=start_date, end=end_date, progress=False)
    
    if stock.empty:
        raise ValueError(f"æ— æ³•è·å– {ticker} çš„æ•°æ®")
    
    # é‡å‘½ååˆ—ä»¥ä¾¿å¤„ç†
    stock.columns = ['Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']
    
    print(f"è·å–åˆ° {len(stock)} ä¸ªäº¤æ˜“æ—¥æ•°æ®")
    return stock

# 2. ç‰¹å¾å·¥ç¨‹ï¼šæ„å»ºäº¤æ˜“é‡ä¸åŠ¨é‡ç‰¹å¾
def create_features(df):
    """
    åˆ›å»ºé¢„æµ‹ç‰¹å¾ï¼ŒåŒ…æ‹¬äº¤æ˜“é‡ç‰¹å¾å’ŒåŠ¨é‡ç‰¹å¾
    """
    df = df.copy()

    # ========== åŸºç¡€ä»·æ ¼ç‰¹å¾ ==========
    df['Returns_1d'] = df['Close'].pct_change(1)  # 1æ—¥æ”¶ç›Šç‡
    df['Returns_5d'] = df['Close'].pct_change(5)  # 5æ—¥åŠ¨é‡ï¼ˆæ ¸å¿ƒç‰¹å¾ï¼‰
    df['Returns_10d'] = df['Close'].pct_change(10)
    df['Returns_20d'] = df['Close'].pct_change(20)

    # ä»·æ ¼ä½ç½®ç‰¹å¾
    df['High_52w'] = df['Close'].rolling(window=252).max()
    df['Low_52w'] = df['Close'].rolling(window=252).min()
    df['Price_Position'] = (df['Close'] - df['Low_52w']) / (df['High_52w'] - df['Low_52w'])

    # ========== äº¤æ˜“é‡ç‰¹å¾ï¼ˆé‡ç‚¹ï¼‰ ==========

    # 1. åŸºç¡€æˆäº¤é‡ç‰¹å¾
    df['Volume_MA_5'] = df['Volume'].rolling(window=5).mean()
    df['Volume_MA_20'] = df['Volume'].rolling(window=20).mean()
    df['Volume_Ratio_5'] = df['Volume'] / df['Volume_MA_5']  # æˆäº¤é‡æ¯”ç‡
    df['Volume_Ratio_20'] = df['Volume'] / df['Volume_MA_20']

    # 2. èµ„é‡‘æµå¼ºåº¦æŒ‡æ ‡ (Money Flow Intensity)
    # å…¸å‹ä»·æ ¼
    df['Typical_Price'] = (df['High'] + df['Low'] + df['Close']) / 3
    # åŸå§‹èµ„é‡‘æµ
    df['Raw_Money_Flow'] = df['Typical_Price'] * df['Volume']

    # æ­£å‘èµ„é‡‘æµï¼ˆä¸Šæ¶¨æ—¥ï¼‰
    df['Positive_MF'] = np.where(df['Close'] > df['Close'].shift(1),
                                  df['Raw_Money_Flow'], 0)
    # è´Ÿå‘èµ„é‡‘æµï¼ˆä¸‹è·Œæ—¥ï¼‰
    df['Negative_MF'] = np.where(df['Close'] < df['Close'].shift(1),
                                  df['Raw_Money_Flow'], 0)

    # è®¡ç®—14æ—¥èµ„é‡‘æµæ¯”ç‡ï¼ˆç±»ä¼¼MFIä½†ç®€åŒ–ï¼‰
    df['Positive_MF_14'] = df['Positive_MF'].rolling(window=14).sum()
    df['Negative_MF_14'] = df['Negative_MF'].rolling(window=14).sum()
    df['MF_Ratio'] = df['Positive_MF_14'] / df['Negative_MF_14']
    df['Money_Flow_Index'] = 100 - (100 / (1 + df['MF_Ratio']))

    # 3. é‡ä»·èƒŒç¦»ç‰¹å¾
    # ä»·æ ¼åˆ›æ–°é«˜ä½†æˆäº¤é‡æœªåˆ›æ–°é«˜ï¼ˆé¡¶èƒŒç¦»ï¼‰
    df['Price_New_High'] = df['Close'] == df['Close'].rolling(window=20).max()
    df['Volume_New_High'] = df['Volume'] == df['Volume'].rolling(window=20).max()
    df['Volume_Price_Divergence'] = np.where(
        (df['Price_New_High'] == True) & (df['Volume_New_High'] == False), 1, 0)

    # 4. æˆäº¤é‡æ³¢åŠ¨ç‡
    df['Volume_Volatility'] = df['Volume'].rolling(window=20).std() / df['Volume_MA_20']

    # 5. ä»·é‡ç›¸å…³æ€§ï¼ˆè¿‡å»20æ—¥ï¼‰
    df['Price_Volume_Corr'] = df['Close'].rolling(window=20).corr(df['Volume'])

    # ========== æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ ==========

    # ç§»åŠ¨å¹³å‡çº¿
    df['MA_5'] = df['Close'].rolling(window=5).mean()
    df['MA_20'] = df['Close'].rolling(window=20).mean()
    df['MA_60'] = df['Close'].rolling(window=60).mean()

    # ä»·æ ¼ä¸å‡çº¿ä½ç½®
    df['Price_vs_MA5'] = (df['Close'] - df['MA_5']) / df['MA_5']
    df['Price_vs_MA20'] = (df['Close'] - df['MA_20']) / df['MA_20']
    df['MA5_vs_MA20'] = (df['MA_5'] - df['MA_20']) / df['MA_20']

    # RSI (ç›¸å¯¹å¼ºå¼±æŒ‡æ•°)
    df['RSI_14'] = ta.momentum.RSIIndicator(df['Close'], window=14).rsi()

    # MACD
    macd = ta.trend.MACD(df['Close'])
    df['MACD'] = macd.macd()
    df['MACD_Signal'] = macd.macd_signal()
    df['MACD_Diff'] = df['MACD'] - df['MACD_Signal']

    # å¸ƒæ—å¸¦
    bb = ta.volatility.BollingerBands(df['Close'])
    df['BB_Upper'] = bb.bollinger_hband()
    df['BB_Lower'] = bb.bollinger_lband()
    df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['Close']
    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])

    # ========== æ³¢åŠ¨ç‡ç‰¹å¾ ==========
    df['Volatility_5d'] = df['Returns_1d'].rolling(window=5).std() * np.sqrt(252)
    df['Volatility_20d'] = df['Returns_1d'].rolling(window=20).std() * np.sqrt(252)

    # ========== å¸‚åœºæƒ…ç»ªç‰¹å¾ ==========
    # è¿ç»­ä¸Šæ¶¨/ä¸‹è·Œå¤©æ•°
    df['Up_Days'] = (df['Close'] > df['Close'].shift(1)).rolling(window=5).sum()
    df['Down_Days'] = (df['Close'] < df['Close'].shift(1)).rolling(window=5).sum()

    # ä»·æ ¼åŠ é€Ÿåº¦ï¼ˆåŠ¨é‡çš„åŠ¨é‡ï¼‰
    df['Momentum_Acceleration'] = df['Returns_5d'] - df['Returns_5d'].shift(5)

    # ========== ç›®æ ‡å˜é‡ï¼šæœªæ¥30å¤©æ”¶ç›Šç‡ ==========
    df['Target_30d'] = df['Close'].pct_change(30).shift(-30)

    # åˆ é™¤åŒ…å«NaNçš„è¡Œï¼ˆç”±äºæ»šåŠ¨è®¡ç®—ï¼‰
    df_clean = df.dropna()

    print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œåˆ›å»ºäº† {df_clean.shape[1] - 1} ä¸ªç‰¹å¾")
    print(f"æœ‰æ•ˆæ•°æ®æ ·æœ¬æ•°: {df_clean.shape[0]}")

    return df_clean

# 3. æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾é€‰æ‹©
def prepare_data(df, test_size=0.2):
    """
    å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
    """
    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    feature_columns = [col for col in df.columns if col not in ['Target_30d', 'Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']]
    X = df[feature_columns]
    y = df['Target_30d']

    # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆæ—¶é—´åºåˆ—æ•°æ®é‡è¦ï¼ï¼‰
    split_idx = int(len(X) * (1 - test_size))

    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    print(f"ç‰¹å¾æ•°é‡: {X_train.shape[1]}")

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, feature_columns

# 4. XGBoostæ¨¡å‹æ„å»ºä¸è®­ç»ƒ
def train_xgboost_model(X_train, y_train, X_test, y_test):
    """
    è®­ç»ƒXGBoostå›å½’æ¨¡å‹
    """
    # å®šä¹‰XGBoostå‚æ•°
    params = {
        'objective': 'reg:squarederror',  # å›å½’ä»»åŠ¡
        'n_estimators': 300,              # æ ‘çš„æ•°é‡
        'learning_rate': 0.05,            # å­¦ä¹ ç‡
        'max_depth': 6,                   # æ ‘çš„æœ€å¤§æ·±åº¦
        'min_child_weight': 1,            # æœ€å°å¶å­èŠ‚ç‚¹æ ·æœ¬æƒé‡å’Œ
        'subsample': 0.8,                 # æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
        'colsample_bytree': 0.8,          # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
        'gamma': 0,                       # åˆ†è£‚æ‰€éœ€çš„æœ€å°æŸå¤±å‡å°‘
        'reg_alpha': 0.1,                 # L1æ­£åˆ™åŒ–
        'reg_lambda': 1,                  # L2æ­£åˆ™åŒ–
        'random_state': 42,
        'n_jobs': -1                      # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
    }

    # åˆ›å»ºæ¨¡å‹
    model = xgb.XGBRegressor(**params)

    # è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")
    model.fit(X_train, y_train,
              eval_set=[(X_train, y_train), (X_test, y_test)],
              eval_metric=['rmse', 'mae'],
              verbose=False,
              early_stopping_rounds=20)

    # é¢„æµ‹
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    return model, y_train_pred, y_test_pred

def evaluate_model(y_true, y_pred, dataset_name="æµ‹è¯•é›†"):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    # å›å½’æŒ‡æ ‡
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)

    # æ–¹å‘å‡†ç¡®æ€§ï¼ˆé¢„æµ‹æ¶¨è·Œæ–¹å‘ï¼‰
    direction_correct = np.sum((y_true > 0) == (y_pred > 0)) / len(y_true) * 100

    # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
    mse_pct = mse * 10000  # è½¬æ¢ä¸ºåŸºç‚¹å¹³æ–¹
    mae_pct = mae * 100    # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
    rmse_pct = rmse * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”

    print(f"\n{'='*50}")
    print(f"{dataset_name} è¯„ä¼°ç»“æœ:")
    print(f"{'='*50}")
    print(f"å‡æ–¹è¯¯å·® (MSE): {mse_pct:.2f} åŸºç‚¹Â²")
    print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae_pct:.2f}%")
    print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse_pct:.2f}%")
    print(f"å†³å®šç³»æ•° (RÂ²): {r2:.4f}")
    print(f"æ–¹å‘å‡†ç¡®æ€§: {direction_correct:.2f}%")

    return {
        'mse': mse, 'mae': mae, 'rmse': rmse,
        'r2': r2, 'direction_accuracy': direction_correct
    }

# 5. ç‰¹å¾é‡è¦æ€§åˆ†æä¸å¯è§†åŒ–
def analyze_feature_importance(model, feature_names, top_n=20):
    """
    åˆ†æç‰¹å¾é‡è¦æ€§
    """
    # è·å–ç‰¹å¾é‡è¦æ€§
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False).reset_index(drop=True)

    print(f"\n{'='*50}")
    print("ç‰¹å¾é‡è¦æ€§æ’å (å‰20å):")
    print('='*50)

    for i, row in importance_df.head(top_n).iterrows():
        print(f"{i+1:2d}. {row['feature']:30s} : {row['importance']:.4f}")

    # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
    plt.figure(figsize=(12, 8))
    top_features = importance_df.head(top_n)
    plt.barh(range(len(top_features)), top_features['importance'][::-1])
    plt.yticks(range(len(top_features)), top_features['feature'][::-1])
    plt.xlabel('ç‰¹å¾é‡è¦æ€§')
    plt.title('XGBoostç‰¹å¾é‡è¦æ€§æ’å (å‰20å)')
    plt.tight_layout()
    plt.show()

    # æŒ‰ç‰¹å¾ç±»åˆ«åˆ†ç»„åˆ†æ
    feature_categories = {
        'åŠ¨é‡ç‰¹å¾': ['Returns_5d', 'Returns_10d', 'Returns_20d', 'Momentum_Acceleration'],
        'äº¤æ˜“é‡ç‰¹å¾': ['Volume_Ratio_5', 'Volume_Ratio_20', 'Money_Flow_Index',
                    'Volume_Price_Divergence', 'Volume_Volatility', 'Price_Volume_Corr'],
        'æŠ€æœ¯æŒ‡æ ‡': ['RSI_14', 'MACD_Diff', 'BB_Width', 'BB_Position',
                  'Price_vs_MA5', 'Price_vs_MA20', 'MA5_vs_MA20'],
        'æ³¢åŠ¨ç‡': ['Volatility_5d', 'Volatility_20d'],
        'å¸‚åœºæƒ…ç»ª': ['Up_Days', 'Down_Days', 'Price_Position']
    }

    category_importance = {}
    for category, features in feature_categories.items():
        # æ‰¾å‡ºå®é™…å­˜åœ¨çš„ç‰¹å¾
        existing_features = [f for f in features if f in importance_df['feature'].values]
        if existing_features:
            category_importance[category] = importance_df[
                importance_df['feature'].isin(existing_features)
            ]['importance'].sum()

    # ç»˜åˆ¶ç±»åˆ«é‡è¦æ€§
    plt.figure(figsize=(10, 6))
    categories = list(category_importance.keys())
    importances = [category_importance[cat] for cat in categories]

    plt.pie(importances, labels=categories, autopct='%1.1f%%', startangle=90)
    plt.axis('equal')
    plt.title('ç‰¹å¾ç±»åˆ«é‡è¦æ€§åˆ†å¸ƒ')
    plt.show()

    return importance_df, category_importance

# 6. é¢„æµ‹ç»“æœå¯è§†åŒ–
def visualize_predictions(y_true, y_pred, dates=None, title="é¢„æµ‹ç»“æœå¯¹æ¯”"):
    """
    å¯è§†åŒ–é¢„æµ‹ç»“æœ
    """
    plt.figure(figsize=(14, 10))

    # å®é™…å€¼ vs é¢„æµ‹å€¼æ•£ç‚¹å›¾
    plt.subplot(2, 2, 1)
    plt.scatter(y_true, y_pred, alpha=0.5)
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
    plt.xlabel('å®é™…30å¤©æ”¶ç›Šç‡')
    plt.ylabel('é¢„æµ‹30å¤©æ”¶ç›Šç‡')
    plt.title('å®é™…å€¼ vs é¢„æµ‹å€¼')
    plt.grid(True, alpha=0.3)

    # é¢„æµ‹è¯¯å·®åˆ†å¸ƒ
    plt.subplot(2, 2, 2)
    errors = y_pred - y_true
    plt.hist(errors, bins=50, edgecolor='black', alpha=0.7)
    plt.xlabel('é¢„æµ‹è¯¯å·®')
    plt.ylabel('é¢‘æ¬¡')
    plt.title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)

    # æ—¶é—´åºåˆ—å¯¹æ¯”ï¼ˆå¦‚æœæœ‰æ—¥æœŸï¼‰
    if dates is not None and len(dates) == len(y_true):
        plt.subplot(2, 1, 2)
        plt.plot(dates, y_true, 'b-', label='å®é™…å€¼', alpha=0.7, linewidth=1)
        plt.plot(dates, y_pred, 'r-', label='é¢„æµ‹å€¼', alpha=0.7, linewidth=1)
        plt.fill_between(dates, y_true, y_pred, alpha=0.2, color='gray')
        plt.xlabel('æ—¥æœŸ')
        plt.ylabel('30å¤©æ”¶ç›Šç‡')
        plt.title('æ—¶é—´åºåˆ—ï¼šå®é™…å€¼ vs é¢„æµ‹å€¼')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)

    plt.tight_layout()
    plt.show()

    # æ–¹å‘å‡†ç¡®æ€§åˆ†æ
    correct_up = np.sum((y_true > 0) & (y_pred > 0))
    correct_down = np.sum((y_true < 0) & (y_pred < 0))
    wrong_up = np.sum((y_true < 0) & (y_pred > 0))
    wrong_down = np.sum((y_true > 0) & (y_pred < 0))

    confusion_data = [[correct_up, wrong_up], [wrong_down, correct_down]]
    confusion_df = pd.DataFrame(confusion_data,
                                index=['å®é™…ä¸Šæ¶¨', 'å®é™…ä¸‹è·Œ'],
                                columns=['é¢„æµ‹ä¸Šæ¶¨', 'é¢„æµ‹ä¸‹è·Œ'])

    print("\né¢„æµ‹æ–¹å‘æ··æ·†çŸ©é˜µ:")
    print(confusion_df)

    # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡
    precision_up = correct_up / (correct_up + wrong_up) if (correct_up + wrong_up) > 0 else 0
    recall_up = correct_up / (correct_up + wrong_down) if (correct_up + wrong_down) > 0 else 0
    precision_down = correct_down / (correct_down + wrong_down) if (correct_down + wrong_down) > 0 else 0
    recall_down = correct_down / (correct_down + wrong_up) if (correct_down + wrong_up) > 0 else 0

    print(f"\nä¸Šæ¶¨é¢„æµ‹ç²¾ç¡®ç‡: {precision_up:.2%}")
    print(f"ä¸Šæ¶¨é¢„æµ‹å¬å›ç‡: {recall_up:.2%}")
    print(f"ä¸‹è·Œé¢„æµ‹ç²¾ç¡®ç‡: {precision_down:.2%}")
    print(f"ä¸‹è·Œé¢„æµ‹å¬å›ç‡: {recall_down:.2%}")

# 7. ä¸»ç¨‹åºï¼šç«¯åˆ°ç«¯é¢„æµ‹æµç¨‹
def main_pipeline(ticker="AAPL", years_of_data=5, test_size=0.2):
    """
    å®Œæ•´çš„ç«¯åˆ°ç«¯é¢„æµ‹æµç¨‹
    """
    print("="*60)
    print(f"è‚¡ç¥¨æœªæ¥30å¤©æ”¶ç›Šç‡é¢„æµ‹ç³»ç»Ÿ")
    print(f"æ ‡çš„: {ticker}")
    print("="*60)

    # 1. è·å–æ•°æ®
    end_date = datetime.now()
    start_date = end_date - timedelta(days=years_of_data*365)

    try:
        raw_data = fetch_stock_data(ticker, start_date, end_date)
    except Exception as e:
        print(f"æ•°æ®è·å–å¤±è´¥: {e}")
        return None

    # 2. ç‰¹å¾å·¥ç¨‹
    print("\n1. è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
    df_with_features = create_features(raw_data)

    # 3. æ•°æ®å‡†å¤‡
    print("\n2. å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®...")
    X_train, X_test, y_train, y_test, scaler, feature_names = prepare_data(
        df_with_features, test_size=test_size)

    # 4. è®­ç»ƒæ¨¡å‹
    print("\n3. è®­ç»ƒXGBoostæ¨¡å‹...")
    model, y_train_pred, y_test_pred = train_xgboost_model(
        X_train, y_train, X_test, y_test)

    # 5. è¯„ä¼°æ¨¡å‹
    print("\n4. è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    train_metrics = evaluate_model(y_train, y_train_pred, "è®­ç»ƒé›†")
    test_metrics = evaluate_model(y_test, y_test_pred, "æµ‹è¯•é›†")

    # 6. ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\n5. åˆ†æç‰¹å¾é‡è¦æ€§...")
    importance_df, category_importance = analyze_feature_importance(
        model, feature_names)

    # 7. å¯è§†åŒ–ç»“æœ
    print("\n6. å¯è§†åŒ–é¢„æµ‹ç»“æœ...")
    test_dates = df_with_features.index[-len(y_test):]
    visualize_predictions(y_test, y_test_pred, test_dates)

    # 8. ç”Ÿæˆé¢„æµ‹æŠ¥å‘Š
    print("\n" + "="*60)
    print("é¢„æµ‹æŠ¥å‘Šæ‘˜è¦")
    print("="*60)

    # æ‰¾å‡ºæœ€é‡è¦çš„ç‰¹å¾
    top_features = importance_df.head(5)['feature'].tolist()

    print(f"\nğŸ” å…³é”®å‘ç°:")
    print(f"1. æœ€é‡è¦çš„5ä¸ªç‰¹å¾: {', '.join(top_features)}")

    # æ£€æŸ¥5æ—¥åŠ¨é‡ç‰¹å¾çš„é‡è¦æ€§
    momentum_rank = importance_df[importance_df['feature'] == 'Returns_5d'].index
    if not momentum_rank.empty:
        rank = momentum_rank[0] + 1
        print(f"2. '5æ—¥åŠ¨é‡'ç‰¹å¾é‡è¦æ€§æ’å: ç¬¬{rank}ä½")

    # æ£€æŸ¥èµ„é‡‘æµç‰¹å¾çš„é‡è¦æ€§
    mfi_rank = importance_df[importance_df['feature'] == 'Money_Flow_Index'].index
    if not mfi_rank.empty:
        rank = mfi_rank[0] + 1
        print(f"3. 'èµ„é‡‘æµæŒ‡æ•°'ç‰¹å¾é‡è¦æ€§æ’å: ç¬¬{rank}ä½")

    print(f"\nğŸ“ˆ æ¨¡å‹è¡¨ç°:")
    print(f"æµ‹è¯•é›†æ–¹å‘å‡†ç¡®æ€§: {test_metrics['direction_accuracy']:.1f}%")
    print(f"æµ‹è¯•é›†RÂ²åˆ†æ•°: {test_metrics['r2']:.4f}")

    # åˆ¤æ–­æ¨¡å‹å®ç”¨æ€§
    if test_metrics['direction_accuracy'] > 55:
        print(f"\nâœ… æ¨¡å‹æ˜¾ç¤ºå‡ºä¸€å®šçš„é¢„æµ‹èƒ½åŠ›")
        if test_metrics['direction_accuracy'] > 60:
            print("   é¢„æµ‹èƒ½åŠ›è¾ƒå¼ºï¼Œå¯è€ƒè™‘ç”¨äºè¾…åŠ©å†³ç­–")
    else:
        print(f"\nâš ï¸  æ¨¡å‹é¢„æµ‹èƒ½åŠ›æœ‰é™ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")

    return {
        'model': model,
        'scaler': scaler,
        'feature_names': feature_names,
        'importance_df': importance_df,
        'test_metrics': test_metrics,
        'predictions': y_test_pred,
        'actuals': y_test
    }

# 8. ä½¿ç”¨ç¤ºä¾‹ä¸æ¨¡å‹éƒ¨ç½²
# ========== ä½¿ç”¨ç¤ºä¾‹ ==========
if __name__ == "__main__":
    # ç¤ºä¾‹1ï¼šåˆ†æè‹¹æœè‚¡ç¥¨
    results = main_pipeline(ticker="AAPL", years_of_data=5, test_size=0.2)

    # ç¤ºä¾‹2ï¼šæ‰¹é‡åˆ†æå¤šåªè‚¡ç¥¨
    # tickers = ["MSFT", "GOOGL", "AMZN", "TSLA"]
    # all_results = {}
    # for ticker in tickers:
    #     print(f"\n{'='*60}")
    #     print(f"åˆ†æ {ticker}")
    #     print('='*60)
    #     try:
    #         all_results[ticker] = main_pipeline(ticker=ticker, years_of_data=3, test_size=0.2)
    #     except Exception as e:
    #         print(f"åˆ†æ{ticker}æ—¶å‡ºé”™: {e}")

    # ç¤ºä¾‹3ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå•ç‚¹é¢„æµ‹
    def predict_single_point(model, scaler, recent_data, feature_names):
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå•ç‚¹é¢„æµ‹
        recent_data: åŒ…å«æœ€è¿‘60ä¸ªäº¤æ˜“æ—¥æ•°æ®çš„DataFrame
        """
        # åˆ›å»ºç‰¹å¾
        df_features = create_features(recent_data)

        if len(df_features) == 0:
            return None

        # è·å–æœ€æ–°æ•°æ®ç‚¹
        latest_features = df_features.iloc[-1:][feature_names]

        # æ ‡å‡†åŒ–
        latest_scaled = scaler.transform(latest_features)

        # é¢„æµ‹
        prediction = model.predict(latest_scaled)[0]

        print(f"\né¢„æµ‹æœªæ¥30å¤©æ”¶ç›Šç‡: {prediction*100:.2f}%")
        if prediction > 0:
            print("é¢„æµ‹æ–¹å‘: ä¸Šæ¶¨ ğŸ“ˆ")
        else:
            print("é¢„æµ‹æ–¹å‘: ä¸‹è·Œ ğŸ“‰")

        return prediction
